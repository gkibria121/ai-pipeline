{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéôÔ∏è Deepfake Audio Detection Training Pipeline\n",
    "\n",
    "A comprehensive deep learning pipeline for detecting AI-generated/synthetic audio (deepfakes) using multiple state-of-the-art model architectures.\n",
    "\n",
    "## üìã Overview\n",
    "\n",
    "| Feature      | Description                                      |\n",
    "| ------------ | ------------------------------------------------ |\n",
    "| **Datasets** | ASVspoof2019, Fake-or-Real, SceneFake            |\n",
    "| **Task**     | Binary classification (Real vs Fake audio)       |\n",
    "| **Features** | Raw waveform, Mel-spectrogram, LFCC, MFCC, CQT   |\n",
    "| **Models**   | EfficientNet-B2, SEResNet, LCNN, RawNet3, AASIST |\n",
    "| **Metrics**  | EER (Equal Error Rate), Accuracy, t-DCF          |\n",
    "\n",
    "## üèóÔ∏è Available Models\n",
    "\n",
    "| Model                           | Parameters | Input Type      | Best For                         |\n",
    "| ------------------------------- | ---------- | --------------- | -------------------------------- |\n",
    "| **EfficientNet-B2 + Attention** | ~9M        | Mel-spectrogram | Maximum accuracy with attention  |\n",
    "| **EfficientNet-B2**             | ~9M        | Mel-spectrogram | Transfer learning from ImageNet  |\n",
    "| **SEResNet**                    | ~12M       | Mel-spectrogram | Channel attention modeling       |\n",
    "| **LCNN**                        | ~0.5M      | Mel-spectrogram | Lightweight & efficient          |\n",
    "| **RawNet3**                     | ~2M        | Raw waveform    | End-to-end learning              |\n",
    "| **AASIST**                      | ~0.3M      | Raw waveform    | State-of-the-art graph attention |\n",
    "\n",
    "## üéØ Key Features\n",
    "\n",
    "‚ú® **7 Model Architectures** - From lightweight LCNN to state-of-the-art AASIST  \n",
    "‚ú® **Multiple Feature Types** - Raw, Mel-spectrogram, LFCC, MFCC, CQT  \n",
    "‚ú® **Data Augmentation** - Random noise, pitch shift, reverberation  \n",
    "‚ú® **Auto Visualization** - Feature analysis and training metrics  \n",
    "‚ú® **Mixed Precision Training** - Faster training with AMP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 1: Setup Environment\n",
    "\n",
    "Clone the repository and navigate to the project directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bYARY7a6H3F",
    "outputId": "fb9d2d44-4ed4-4896-a9d5-2098f4df354b"
   },
   "outputs": [],
   "source": [
    "%cd /content\n",
    "!git clone --branch add_fake_or_real_dataset https://github.com/gkibria121/ai-pipeline.git\n",
    "%cd ai-pipeline\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 2: Install Dependencies\n",
    "\n",
    "Install all required Python packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wlHSCI6gdSWG",
    "outputId": "90eb7b94-115e-4ef2-c47a-38121681a3a8"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9Di0sKyb6p1"
   },
   "source": [
    "## üìñ Training Configuration Guide\n",
    "\n",
    "### Dataset Selection (`--dataset`)\n",
    "\n",
    "| Flag | Dataset      | Description                                     |\n",
    "| ---- | ------------ | ----------------------------------------------- |\n",
    "| `1`  | ASVspoof2019 | Standard benchmark for audio spoofing detection |\n",
    "| `2`  | Fake-or-Real | Binary classification for fake vs real audio    |\n",
    "| `3`  | SceneFake    | Scene-aware fake audio detection                |\n",
    "\n",
    "### Feature Type Options (`--feature_type`)\n",
    "\n",
    "| Flag | Feature         | Best With                    | Description                            |\n",
    "| ---- | --------------- | ---------------------------- | -------------------------------------- |\n",
    "| `0`  | Raw waveform    | RawNet3, AASIST              | Direct waveform processing             |\n",
    "| `1`  | Mel-spectrogram | EfficientNet, SEResNet, LCNN | 128 mel bins, best for CNN models      |\n",
    "| `2`  | LFCC            | All models                   | Linear Frequency Cepstral Coefficients |\n",
    "| `3`  | MFCC            | All models                   | Mel-Frequency Cepstral Coefficients    |\n",
    "| `4`  | CQT             | All models                   | Constant-Q Transform                   |\n",
    "\n",
    "### Command Line Arguments\n",
    "\n",
    "```bash\n",
    "python main.py \\\n",
    "    --config <config_file>      # Model configuration file\n",
    "    --dataset <1|2|3>           # Dataset to use\n",
    "    --feature_type <0-4>        # Audio feature representation\n",
    "    --epochs <num>              # Number of training epochs\n",
    "    --batch_size <num>          # Batch size (optional, overrides config)\n",
    "    --random_noise              # Enable data augmentation\n",
    "    --eval                      # Evaluation mode only\n",
    "    --eval_model_weights <path> # Path to model weights for evaluation\n",
    "    --data_subset <0.0-1.0>     # Use subset of data (for quick testing)\n",
    "```\n",
    "\n",
    "### Quick Examples\n",
    "\n",
    "```bash\n",
    "# Train EfficientNet-B2 on Fake-or-Real with augmentation\n",
    "python main.py --config config/EfficientNetB2.conf --dataset 2 --feature_type 1 --epochs 20 --random_noise\n",
    "\n",
    "# Train AASIST on ASVspoof2019\n",
    "python main.py --config config/AASIST.conf --dataset 1 --feature_type 0 --epochs 50\n",
    "\n",
    "# Quick test with 10% of data\n",
    "python main.py --config config/LCNN.conf --dataset 2 --feature_type 1 --epochs 5 --data_subset 0.1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Step 3: Download Fake-or-Real Dataset\n",
    "\n",
    "Download the Fake-or-Real dataset (2-second audio clips). This contains:\n",
    "\n",
    "- **Training**: 13,956 samples (6,978 real + 6,978 fake)\n",
    "- **Validation**: 2,826 samples (1,413 real + 1,413 fake)\n",
    "- **Testing**: 1,088 samples (544 real + 544 fake)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python download_dataset.py --dataset 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Model Training - Recommended Configurations\n",
    "\n",
    "Train with the best feature types for each model architecture.\n",
    "**Ordered by model complexity**: Heavy models first (longer training) ‚Üí Lighter models last (faster training)\n",
    "\n",
    "### 1. üé™ EfficientNet-B2 with Attention (Heaviest - ~9M params + Attention)\n",
    "\n",
    "**Best for**: Maximum performance with spatial attention\n",
    "\n",
    "- Attention-weighted pooling\n",
    "- Better temporal modeling\n",
    "- **Recommended**: `feature_type 1` (Mel-Spectrogram)\n",
    "- **Training time**: Longest (~25 epochs recommended)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --config config/EfficientNetB2_Attention.conf --feature_type 1 --dataset 2 --epochs 25 --random_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --config config/EfficientNetB2_Attention.conf --feature_type 1 --dataset 2 --epochs 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ‚ö° EfficientNet-B2 Standard (Heavy - ~9M params)\n",
    "\n",
    "**Best for**: Pre-trained ImageNet knowledge transfer\n",
    "\n",
    "- Compound scaling for efficiency\n",
    "- State-of-the-art CNN architecture\n",
    "- **Recommended**: `feature_type 1` (Mel-Spectrogram)\n",
    "- **Training time**: Long (~20 epochs recommended)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --config config/EfficientNetB2.conf --feature_type 1 --dataset 2 --epochs 20 --random_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --config config/EfficientNetB2.conf --feature_type 1 --dataset 2 --epochs 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. üî• SEResNet (Medium - ~12M params)\n",
    "\n",
    "**Best for**: Time-frequency representation learning\n",
    "\n",
    "- SE blocks for channel attention\n",
    "- Attentive statistics pooling\n",
    "- **Recommended**: `feature_type 1` (Mel-Spectrogram)\n",
    "- **Training time**: Medium (~15 epochs recommended)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --config config/SEResNet.conf --feature_type 1 --dataset 2 --epochs 15 --random_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --config config/SEResNet.conf --feature_type 1 --dataset 2 --epochs 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. üî≤ LCNN (Light CNN with MFM - Lightweight ~0.5M params)\n",
    "\n",
    "**Best for**: Efficient and robust deepfake detection\n",
    "\n",
    "- Max-Feature-Map (MFM) activation for noise suppression\n",
    "- Residual blocks for better gradient flow\n",
    "- Attentive statistics pooling\n",
    "- **Recommended**: `feature_type 1` (Mel-Spectrogram)\n",
    "- **Training time**: Fast (~20 epochs recommended)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --config config/LCNN.conf --feature_type 1 --dataset 2 --epochs 20 --random_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --config config/LCNN.conf --feature_type 1 --dataset 2 --epochs 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. üéØ RawNet3 (End-to-end - ~2M params)\n",
    "\n",
    "**Best for**: Fast end-to-end learning from raw audio\n",
    "\n",
    "- Learnable SincConv filters for adaptive frequency response\n",
    "- Res2Net blocks for multi-scale feature extraction\n",
    "- Processes raw waveform directly (no preprocessing needed)\n",
    "- **Recommended**: `feature_type 0` (Raw waveform)\n",
    "- **Training time**: Fast (~15 epochs recommended)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --config config/RawNet3.conf --feature_type 0 --dataset 2 --epochs 15 --random_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --config config/RawNet3.conf --feature_type 0 --dataset 2 --epochs 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. üß† AASIST (Graph Attention Network - ~0.3M params)\n",
    "\n",
    "**Best for**: State-of-the-art detection accuracy\n",
    "\n",
    "- Graph Attention Layers for spectro-temporal modeling\n",
    "- Heterogeneous stacking for multi-scale features\n",
    "- Winner architecture in ASVspoof challenges\n",
    "- **Recommended**: `feature_type 0` (Raw waveform)\n",
    "- **Training time**: Medium-Long (~25-50 epochs recommended)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --config config/AASIST.conf --feature_type 0 --dataset 2 --epochs 25 --random_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --config config/AASIST.conf --feature_type 0 --dataset 2 --epochs 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Training Results & Outputs\n",
    "\n",
    "After training, results are automatically saved in organized folders:\n",
    "\n",
    "```\n",
    "results/\n",
    "‚îî‚îÄ‚îÄ FakeOrReal_audio_LCNN_ep20_bs32_feat1/\n",
    "    ‚îú‚îÄ‚îÄ config.conf              # Copy of training config\n",
    "    ‚îú‚îÄ‚îÄ weights/                 # Model checkpoints\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ best.pth            # Best model (lowest dev EER)\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ swa.pth             # SWA averaged model\n",
    "    ‚îú‚îÄ‚îÄ metrics/                 # Training metrics\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ epoch_metrics.json  # Per-epoch metrics\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ final_summary.json  # Final results\n",
    "    ‚îú‚îÄ‚îÄ metric_log.txt          # Training log\n",
    "    ‚îú‚îÄ‚îÄ evaluation_results.txt  # Final evaluation\n",
    "    ‚îî‚îÄ‚îÄ events.out.*            # TensorBoard logs\n",
    "```\n",
    "\n",
    "### Key Metrics\n",
    "\n",
    "| Metric       | Description                    | Good Value            |\n",
    "| ------------ | ------------------------------ | --------------------- |\n",
    "| **EER**      | Equal Error Rate               | < 5%                  |\n",
    "| **Accuracy** | Classification accuracy        | > 95%                 |\n",
    "| **t-DCF**    | Tandem Detection Cost Function | < 0.1 (ASVspoof only) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Model Evaluation\n",
    "\n",
    "After training, evaluate your model on the test set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate a trained model (replace path with your trained model)\n",
    "# !python main.py --config config/LCNN.conf --dataset 2 --feature_type 1 --eval --eval_model_weights ./results/FakeOrReal_audio_LCNN_ep20_bs32_feat1/weights/best.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Tips & Best Practices\n",
    "\n",
    "### Choosing the Right Model\n",
    "\n",
    "| Use Case              | Recommended Model                   | Why                          |\n",
    "| --------------------- | ----------------------------------- | ---------------------------- |\n",
    "| **Quick prototyping** | LCNN                                | Fast training, good accuracy |\n",
    "| **Production (edge)** | LCNN, RawNet3                       | Lightweight, efficient       |\n",
    "| **Maximum accuracy**  | AASIST, EfficientNet-B2 + Attention | State-of-the-art performance |\n",
    "| **Transfer learning** | EfficientNet-B2                     | Pretrained ImageNet weights  |\n",
    "\n",
    "### Training Tips\n",
    "\n",
    "1. **Start with augmentation** (`--random_noise`) - Usually improves generalization\n",
    "2. **Use Mel-spectrogram** (`--feature_type 1`) for CNN-based models\n",
    "3. **Use Raw waveform** (`--feature_type 0`) for RawNet3 and AASIST\n",
    "4. **Monitor validation EER** - Lower is better (0% = perfect)\n",
    "5. **Use `--data_subset 0.1`** for quick experiments before full training\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "| Issue         | Solution                                             |\n",
    "| ------------- | ---------------------------------------------------- |\n",
    "| Out of memory | Reduce `batch_size` in config or via `--batch_size`  |\n",
    "| Slow training | Enable `use_amp: true` in config for mixed precision |\n",
    "| Overfitting   | Add `--random_noise` or increase `dropout` in config |\n",
    "| Poor results  | Try different `feature_type` or more epochs          |\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
