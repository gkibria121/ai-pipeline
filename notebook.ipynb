{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéôÔ∏è Deepfake Audio Detection Pipeline\n",
    "\n",
    "A deep learning pipeline for detecting AI-generated/synthetic audio using multiple model architectures.\n",
    "\n",
    "## üìã Overview\n",
    "\n",
    "| Feature      | Description                                      |\n",
    "| ------------ | ------------------------------------------------ |\n",
    "| **Datasets** | ASVspoof2019, Fake-or-Real, SceneFake            |\n",
    "| **Task**     | Binary classification (Real vs Fake audio)       |\n",
    "| **Features** | Raw waveform, Mel-spectrogram, LFCC, MFCC, CQT   |\n",
    "| **Models**   | EfficientNet-B2, SEResNet, LCNN, RawNet3, AASIST |\n",
    "| **Metrics**  | EER (Equal Error Rate), Accuracy, t-DCF          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 1: Setup Environment\n",
    "\n",
    "Clone the repository and navigate to the project directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bYARY7a6H3F",
    "outputId": "fb9d2d44-4ed4-4896-a9d5-2098f4df354b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "fatal: destination path 'ai-pipeline' already exists and is not an empty directory.\n",
      "/content/ai-pipeline\n",
      "remote: Enumerating objects: 3, done.\u001b[K\n",
      "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
      "remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects: 100% (3/3), 1.10 KiB | 1.10 MiB/s, done.\n",
      "From https://github.com/gkibria121/ai-pipeline\n",
      "   37f96c9..7c5477e  add_fake_or_real_dataset_windows -> origin/add_fake_or_real_dataset_windows\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "%cd /content\n",
    "!git clone --branch add_fake_or_real_dataset https://github.com/gkibria121/ai-pipeline.git\n",
    "%cd ai-pipeline\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 2: Install Dependencies\n",
    "\n",
    "Install all required Python packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wlHSCI6gdSWG",
    "outputId": "90eb7b94-115e-4ef2-c47a-38121681a3a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.9.0+cu126)\n",
      "Collecting torchcontrib (from -r requirements.txt (line 2))\n",
      "  Downloading torchcontrib-0.0.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (2.0.2)\n",
      "Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (0.13.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (4.67.1)\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (0.11.0)\n",
      "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (0.3.13)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (3.10.0)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (2.19.0)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (0.13.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (2.2.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile->-r requirements.txt (line 4)) (2.0.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 6)) (3.1.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 6)) (0.60.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 6)) (1.16.3)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 6)) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 6)) (1.5.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 6)) (4.4.2)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 6)) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 6)) (1.0.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 6)) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->-r requirements.txt (line 6)) (1.1.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub->-r requirements.txt (line 7)) (25.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub->-r requirements.txt (line 7)) (6.0.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub->-r requirements.txt (line 7)) (2.32.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (2.9.0.post0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 9)) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 9)) (1.76.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 9)) (3.10)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 9)) (5.29.5)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 9)) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 9)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 9)) (3.1.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 11)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 11)) (2025.2)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile->-r requirements.txt (line 4)) (2.23)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa->-r requirements.txt (line 6)) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa->-r requirements.txt (line 6)) (4.5.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->-r requirements.txt (line 7)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->-r requirements.txt (line 7)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->-r requirements.txt (line 7)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->-r requirements.txt (line 7)) (2025.11.12)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa->-r requirements.txt (line 6)) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 9)) (3.0.3)\n",
      "Building wheels for collected packages: torchcontrib\n",
      "  Building wheel for torchcontrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for torchcontrib: filename=torchcontrib-0.0.2-py3-none-any.whl size=7516 sha256=002c62c43b493f9b1d8fa02c693eecaaa6c2c7c6ad4be123f878f965deab9cab\n",
      "  Stored in directory: /root/.cache/pip/wheels/e3/d1/1f/63f00ffea223db446943147a04ff035eb40d00cec3e87d63e5\n",
      "Successfully built torchcontrib\n",
      "Installing collected packages: torchcontrib\n",
      "Successfully installed torchcontrib-0.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9Di0sKyb6p1"
   },
   "source": [
    "## üìñ Available Options\n",
    "\n",
    "### Models (`--config`)\n",
    "\n",
    "| Config File                            | Model                       | Parameters | Input Type   |\n",
    "| -------------------------------------- | --------------------------- | ---------- | ------------ |\n",
    "| `config/LCNN.conf`                     | LCNN                        | ~0.5M      | Spectrogram  |\n",
    "| `config/LCNN_Large.conf`               | LCNN Large                  | ~1M        | Spectrogram  |\n",
    "| `config/SEResNet.conf`                 | SEResNet                    | ~12M       | Spectrogram  |\n",
    "| `config/EfficientNetB2.conf`           | EfficientNet-B2             | ~9M        | Spectrogram  |\n",
    "| `config/EfficientNetB2_Attention.conf` | EfficientNet-B2 + Attention | ~9M        | Spectrogram  |\n",
    "| `config/RawNet3.conf`                  | RawNet3                     | ~2M        | Raw waveform |\n",
    "| `config/AASIST.conf`                   | AASIST                      | ~0.3M      | Raw waveform |\n",
    "| `config/AASIST-L.conf`                 | AASIST-L                    | ~0.6M      | Raw waveform |\n",
    "\n",
    "### Datasets (`--dataset`)\n",
    "\n",
    "| Flag | Dataset      | Description                                     |\n",
    "| ---- | ------------ | ----------------------------------------------- |\n",
    "| `1`  | ASVspoof2019 | Standard benchmark for audio spoofing detection |\n",
    "| `2`  | Fake-or-Real | Binary classification for fake vs real audio    |\n",
    "| `3`  | SceneFake    | Scene-aware fake audio detection                |\n",
    "\n",
    "### Feature Types (`--feature_type`)\n",
    "\n",
    "| Flag | Feature         | Description                                      |\n",
    "| ---- | --------------- | ------------------------------------------------ |\n",
    "| `0`  | Raw waveform    | Direct waveform processing (for RawNet3, AASIST) |\n",
    "| `1`  | Mel-spectrogram | 128 mel bins                                     |\n",
    "| `2`  | LFCC            | Linear Frequency Cepstral Coefficients           |\n",
    "| `3`  | MFCC            | Mel-Frequency Cepstral Coefficients              |\n",
    "| `4`  | CQT             | Constant-Q Transform                             |\n",
    "\n",
    "### Command Line Arguments\n",
    "\n",
    "```bash\n",
    "python main.py \\\n",
    "    --config <config_file>      # Model configuration file\n",
    "    --dataset <1|2|3>           # Dataset to use\n",
    "    --feature_type <0-4>        # Audio feature representation\n",
    "    --epochs <num>              # Number of training epochs\n",
    "    --batch_size <num>          # Batch size (overrides config)\n",
    "    --random_noise              # Enable data augmentation\n",
    "    --weight_avg                # Enable Stochastic Weight Averaging (SWA)\n",
    "    --eval_best                 # Evaluate on test set when best model is found\n",
    "    --eval                      # Evaluation mode only\n",
    "    --eval_model_weights <path> # Path to model weights for evaluation\n",
    "    --data_subset <0.0-1.0>     # Use subset of data (for quick testing)\n",
    "```\n",
    "\n",
    "### Training Flags\n",
    "\n",
    "| Flag             | Description                                                  |\n",
    "| ---------------- | ------------------------------------------------------------ |\n",
    "| `--random_noise` | Enable data augmentation (RIR, MUSAN, pitch shift, etc.)     |\n",
    "| `--weight_avg`   | Enable Stochastic Weight Averaging for better generalization |\n",
    "| `--eval_best`    | Evaluate on test set each time a new best model is found     |\n",
    "\n",
    "### Augmentation Types (`--random_noise`)\n",
    "\n",
    "When enabled, applies these augmentations randomly:\n",
    "\n",
    "| Augmentation      | Description                                      |\n",
    "| ----------------- | ------------------------------------------------ |\n",
    "| RIR Simulation    | Room Impulse Response - simulates room acoustics |\n",
    "| MUSAN-style Noise | Babble, music, and ambient noise                 |\n",
    "| Gaussian Noise    | Additive white Gaussian noise (SNR: 10-25 dB)    |\n",
    "| Reverberation     | Echo/reverb effects                              |\n",
    "| Pitch Shift       | ¬±4 semitones                                     |\n",
    "| Time Stretch      | 0.85x - 1.15x speed                              |\n",
    "| Gain              | ¬±6 dB volume adjustment                          |\n",
    "| Filters           | Low-pass and high-pass filtering                 |\n",
    "| SpecAugment       | Frequency and time masking for spectrograms      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Step 3: Download Fake-or-Real Dataset\n",
    "\n",
    "Download the Fake-or-Real dataset (2-second audio clips). This contains:\n",
    "\n",
    "- **Training**: 13,956 samples (6,978 real + 6,978 fake)\n",
    "- **Validation**: 2,826 samples (1,413 real + 1,413 fake)\n",
    "- **Testing**: 1,088 samples (544 real + 544 fake)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATASET DOWNLOADER\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Downloading Fake-or-Real Dataset\n",
      "======================================================================\n",
      "Using Colab cache for faster access to the 'the-fake-or-real-dataset' dataset.\n",
      "‚úì Data source download complete.\n",
      "‚úì Symlink created: ./fake_or_real ‚Üí /kaggle/input/the-fake-or-real-dataset\n",
      "‚úì Fake-or-Real dataset ready!\n",
      "\n",
      "======================================================================\n",
      "Download Complete!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "!python download_dataset.py --dataset 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Step 4: Train Models\n",
    "\n",
    "### LCNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -u main.py --config config/LCNN.conf --feature_type 1 --dataset 2 --epochs 20 --random_noise --weight_avg --eval_best "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCNN Large\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -u main.py --config config/LCNN_Large.conf --feature_type 1 --dataset 2 --epochs 20 --random_noise --weight_avg --eval_best "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEResNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-11 15:26:21.936692: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765466781.956908    2177 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765466781.963253    2177 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765466781.978338    2177 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765466781.978364    2177 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765466781.978368    2177 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765466781.978371    2177 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-11 15:26:21.982974: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "======================================================================\n",
      "DATASET: Fake-or-Real (Type 2)\n",
      "======================================================================\n",
      "‚úÖ Eval on best: ENABLED (will evaluate on test set when best model found)\n",
      "‚úÖ Random augmentation: ENABLED (RIR, MUSAN-style noise, pitch shift, time stretch, SpecAugment)\n",
      "‚úÖ Weight averaging (SWA): ENABLED\n",
      "\n",
      "==================================================\n",
      "Feature Type: 1 (MEL_SPECTROGRAM)\n",
      "Generating feature analysis visualization...\n",
      "==================================================\n",
      "Using sample: file14641.mp3.wav_16k.wav_norm.wav_mono.wav_silence.wav_2sec.wav\n",
      "‚úì Feature analysis saved to: exp_result/FakeorReal_audio_SEResNet_rand_ep15_bs32_feat1/feature_analysis/feature_analysis_mel_spectrogram.png\n",
      "Device: cuda\n",
      "GPU: Tesla T4 (Compute Capability: 7.5)\n",
      "BF16 Native Support: False\n",
      "  ‚Üí Using FP16 mixed precision (BF16 requires Ampere+ GPU)\n",
      "TF32 Enabled: True\n",
      "CuDNN Benchmark: Enabled\n",
      "no. model params:11981251\n",
      "Memory Format: contiguous (channels_last disabled for GPU compatibility)\n",
      "‚ö†Ô∏è  Disabling torch.compile on this GPU (T4/V100 have slow compilation)\n",
      "\n",
      "============================================================\n",
      "TRAINING CONFIGURATION SUMMARY\n",
      "============================================================\n",
      "  Model:            SEResNet\n",
      "  Dataset:          Fake-or-Real (Type 2)\n",
      "  Feature:          mel_spectrogram (Type 1)\n",
      "  Augmentation:     ‚úÖ ENABLED\n",
      "  Weight Avg (SWA): ‚úÖ ENABLED\n",
      "  Epochs:           15\n",
      "  Batch Size:       32\n",
      "============================================================\n",
      "\n",
      "\n",
      "Dataset loaded from: fake_or_real/for-2sec/for-2seconds\n",
      "Training samples: 13956 (Real: 6978, Fake: 6978)\n",
      "Validation samples: 2826 (Real: 1413, Fake: 1413)\n",
      "Testing samples: 1088 (Real: 544, Fake: 544)\n",
      "\n",
      "\n",
      "üìä Using 1.0% data subset:\n",
      "  Training: 139 samples\n",
      "  Validation: 28 samples\n",
      "  Testing: 10 samples\n",
      "\n",
      "\n",
      "==================================================\n",
      "Start training epoch 1/15\n",
      "==================================================\n",
      "Training: 100% 4/4 [00:15<00:00,  3.96s/batch, loss=1.16081]\n",
      "\n",
      "Validating on development set...\n",
      "Evaluation: 100% 1/1 [00:00<00:00,  1.48batch/s]\n",
      "Scores saved to exp_result/FakeorReal_audio_SEResNet_rand_ep15_bs32_feat1/metrics/dev_score.txt\n",
      "\\nCLASSIFICATION RESULTS\\n\\tEER\\t\\t= 39.444444444 % (Equal error rate)\\n\\tAccuracy\\t=  57.1429 % (Classification accuracy at EER threshold)\\nDONE.\n",
      "Loss:1.16081, dev_eer: 39.444, dev_acc: 57.14%\n",
      "\n",
      "==================================================\n",
      "Start training epoch 2/15\n",
      "==================================================\n",
      "Training: 100% 4/4 [00:03<00:00,  1.33batch/s, loss=0.98568]\n",
      "\n",
      "Validating on development set...\n",
      "Evaluation: 100% 1/1 [00:00<00:00,  4.23batch/s]\n",
      "Scores saved to exp_result/FakeorReal_audio_SEResNet_rand_ep15_bs32_feat1/metrics/dev_score.txt\n",
      "\\nCLASSIFICATION RESULTS\\n\\tEER\\t\\t= 39.444444444 % (Equal error rate)\\n\\tAccuracy\\t=  64.2857 % (Classification accuracy at EER threshold)\\nDONE.\n",
      "Loss:0.98568, dev_eer: 39.444, dev_acc: 64.29%\n",
      "\n",
      "==================================================\n",
      "Start training epoch 3/15\n",
      "==================================================\n",
      "Training: 100% 4/4 [00:01<00:00,  2.20batch/s, loss=0.95799]\n",
      "\n",
      "Validating on development set...\n",
      "Evaluation: 100% 1/1 [00:00<00:00,  4.23batch/s]\n",
      "Scores saved to exp_result/FakeorReal_audio_SEResNet_rand_ep15_bs32_feat1/metrics/dev_score.txt\n",
      "\\nCLASSIFICATION RESULTS\\n\\tEER\\t\\t= 39.444444444 % (Equal error rate)\\n\\tAccuracy\\t=  64.2857 % (Classification accuracy at EER threshold)\\nDONE.\n",
      "Loss:0.95799, dev_eer: 39.444, dev_acc: 64.29%\n",
      "\n",
      "==================================================\n",
      "Start training epoch 4/15\n",
      "==================================================\n",
      "Training: 100% 4/4 [00:01<00:00,  2.02batch/s, loss=0.85136]\n",
      "\n",
      "Validating on development set...\n",
      "Evaluation: 100% 1/1 [00:00<00:00,  4.39batch/s]\n",
      "Scores saved to exp_result/FakeorReal_audio_SEResNet_rand_ep15_bs32_feat1/metrics/dev_score.txt\n",
      "\\nCLASSIFICATION RESULTS\\n\\tEER\\t\\t= 39.444444444 % (Equal error rate)\\n\\tAccuracy\\t=  64.2857 % (Classification accuracy at EER threshold)\\nDONE.\n",
      "Loss:0.85136, dev_eer: 39.444, dev_acc: 64.29%\n",
      "\n",
      "==================================================\n",
      "Start training epoch 5/15\n",
      "==================================================\n",
      "Training: 100% 4/4 [00:01<00:00,  2.02batch/s, loss=0.49396]\n",
      "\n",
      "Validating on development set...\n",
      "Evaluation: 100% 1/1 [00:00<00:00,  4.14batch/s]\n",
      "Scores saved to exp_result/FakeorReal_audio_SEResNet_rand_ep15_bs32_feat1/metrics/dev_score.txt\n",
      "\\nCLASSIFICATION RESULTS\\n\\tEER\\t\\t= 39.444444444 % (Equal error rate)\\n\\tAccuracy\\t=  64.2857 % (Classification accuracy at EER threshold)\\nDONE.\n",
      "Loss:0.49396, dev_eer: 39.444, dev_acc: 64.29%\n",
      "\n",
      "==================================================\n",
      "Start training epoch 6/15\n",
      "==================================================\n",
      "Training: 100% 4/4 [00:01<00:00,  2.14batch/s, loss=0.47603]\n",
      "\n",
      "Validating on development set...\n",
      "Evaluation: 100% 1/1 [00:00<00:00,  4.36batch/s]\n",
      "Scores saved to exp_result/FakeorReal_audio_SEResNet_rand_ep15_bs32_feat1/metrics/dev_score.txt\n",
      "\\nCLASSIFICATION RESULTS\\n\\tEER\\t\\t= 39.444444444 % (Equal error rate)\\n\\tAccuracy\\t=  64.2857 % (Classification accuracy at EER threshold)\\nDONE.\n",
      "Loss:0.47603, dev_eer: 39.444, dev_acc: 64.29%\n",
      "\n",
      "==================================================\n",
      "Start training epoch 7/15\n",
      "==================================================\n",
      "Training: 100% 4/4 [00:03<00:00,  1.23batch/s, loss=0.27215]\n",
      "\n",
      "Validating on development set...\n",
      "Evaluation: 100% 1/1 [00:00<00:00,  3.69batch/s]\n",
      "Scores saved to exp_result/FakeorReal_audio_SEResNet_rand_ep15_bs32_feat1/metrics/dev_score.txt\n",
      "\\nCLASSIFICATION RESULTS\\n\\tEER\\t\\t= 39.444444444 % (Equal error rate)\\n\\tAccuracy\\t=  64.2857 % (Classification accuracy at EER threshold)\\nDONE.\n",
      "Loss:0.27215, dev_eer: 39.444, dev_acc: 64.29%\n",
      "\n",
      "==================================================\n",
      "Start training epoch 8/15\n",
      "==================================================\n",
      "Training: 100% 4/4 [00:02<00:00,  1.92batch/s, loss=0.50132]\n",
      "\n",
      "Validating on development set...\n",
      "Evaluation: 100% 1/1 [00:00<00:00,  4.17batch/s]\n",
      "Scores saved to exp_result/FakeorReal_audio_SEResNet_rand_ep15_bs32_feat1/metrics/dev_score.txt\n",
      "\\nCLASSIFICATION RESULTS\\n\\tEER\\t\\t= 39.444444444 % (Equal error rate)\\n\\tAccuracy\\t=  64.2857 % (Classification accuracy at EER threshold)\\nDONE.\n",
      "Loss:0.50132, dev_eer: 39.444, dev_acc: 64.29%\n",
      "Saving epoch 8 for SWA (weight averaging - late epoch)\n",
      "\n",
      "==================================================\n",
      "Start training epoch 9/15\n",
      "==================================================\n",
      "Training: 100% 4/4 [00:02<00:00,  1.98batch/s, loss=0.57628]\n",
      "\n",
      "Validating on development set...\n",
      "Evaluation: 100% 1/1 [00:00<00:00,  4.11batch/s]\n",
      "Scores saved to exp_result/FakeorReal_audio_SEResNet_rand_ep15_bs32_feat1/metrics/dev_score.txt\n",
      "\\nCLASSIFICATION RESULTS\\n\\tEER\\t\\t= 39.444444444 % (Equal error rate)\\n\\tAccuracy\\t=  64.2857 % (Classification accuracy at EER threshold)\\nDONE.\n",
      "Loss:0.57628, dev_eer: 39.444, dev_acc: 64.29%\n",
      "Saving epoch 9 for SWA (weight averaging - late epoch)\n",
      "\n",
      "==================================================\n",
      "Start training epoch 10/15\n",
      "==================================================\n",
      "Training: 100% 4/4 [00:01<00:00,  2.08batch/s, loss=0.20309]\n",
      "\n",
      "Validating on development set...\n",
      "Evaluation: 100% 1/1 [00:00<00:00,  3.98batch/s]\n",
      "Scores saved to exp_result/FakeorReal_audio_SEResNet_rand_ep15_bs32_feat1/metrics/dev_score.txt\n",
      "\\nCLASSIFICATION RESULTS\\n\\tEER\\t\\t= 28.888888889 % (Equal error rate)\\n\\tAccuracy\\t=  67.8571 % (Classification accuracy at EER threshold)\\nDONE.\n",
      "Loss:0.20309, dev_eer: 28.889, dev_acc: 67.86%\n",
      "Saving epoch 10 for SWA (weight averaging - late epoch)\n",
      "\n",
      "==================================================\n",
      "Start training epoch 11/15\n",
      "==================================================\n",
      "Training: 100% 4/4 [00:02<00:00,  1.98batch/s, loss=0.21156]\n",
      "\n",
      "Validating on development set...\n",
      "Evaluation: 100% 1/1 [00:00<00:00,  4.27batch/s]\n",
      "Scores saved to exp_result/FakeorReal_audio_SEResNet_rand_ep15_bs32_feat1/metrics/dev_score.txt\n",
      "\\nCLASSIFICATION RESULTS\\n\\tEER\\t\\t= 21.111111111 % (Equal error rate)\\n\\tAccuracy\\t=  75.0000 % (Classification accuracy at EER threshold)\\nDONE.\n",
      "Loss:0.21156, dev_eer: 21.111, dev_acc: 75.00%\n",
      "Saving epoch 11 for SWA (weight averaging - late epoch)\n",
      "\n",
      "==================================================\n",
      "Start training epoch 12/15\n",
      "==================================================\n",
      "Training: 100% 4/4 [00:03<00:00,  1.20batch/s, loss=0.26548]\n",
      "\n",
      "Validating on development set...\n",
      "Evaluation: 100% 1/1 [00:00<00:00,  3.79batch/s]\n",
      "Scores saved to exp_result/FakeorReal_audio_SEResNet_rand_ep15_bs32_feat1/metrics/dev_score.txt\n",
      "\\nCLASSIFICATION RESULTS\\n\\tEER\\t\\t= 21.111111111 % (Equal error rate)\\n\\tAccuracy\\t=  82.1429 % (Classification accuracy at EER threshold)\\nDONE.\n",
      "Loss:0.26548, dev_eer: 21.111, dev_acc: 82.14%\n",
      "Saving epoch 12 for SWA (weight averaging - late epoch)\n",
      "\n",
      "==================================================\n",
      "Start training epoch 13/15\n",
      "==================================================\n",
      "Training: 100% 4/4 [00:01<00:00,  2.06batch/s, loss=0.32399]\n",
      "\n",
      "Validating on development set...\n",
      "Evaluation: 100% 1/1 [00:00<00:00,  4.33batch/s]\n",
      "Scores saved to exp_result/FakeorReal_audio_SEResNet_rand_ep15_bs32_feat1/metrics/dev_score.txt\n",
      "\\nCLASSIFICATION RESULTS\\n\\tEER\\t\\t= 10.555555556 % (Equal error rate)\\n\\tAccuracy\\t=  92.8571 % (Classification accuracy at EER threshold)\\nDONE.\n",
      "Loss:0.32399, dev_eer: 10.556, dev_acc: 92.86%\n",
      "Saving epoch 13 for SWA (weight averaging - late epoch)\n",
      "\n",
      "==================================================\n",
      "Start training epoch 14/15\n",
      "==================================================\n",
      "Training: 100% 4/4 [00:01<00:00,  2.02batch/s, loss=0.19090]\n",
      "\n",
      "Validating on development set...\n",
      "Evaluation: 100% 1/1 [00:00<00:00,  4.42batch/s]\n",
      "Scores saved to exp_result/FakeorReal_audio_SEResNet_rand_ep15_bs32_feat1/metrics/dev_score.txt\n",
      "\\nCLASSIFICATION RESULTS\\n\\tEER\\t\\t= 10.555555556 % (Equal error rate)\\n\\tAccuracy\\t=  92.8571 % (Classification accuracy at EER threshold)\\nDONE.\n",
      "Loss:0.19090, dev_eer: 10.556, dev_acc: 92.86%\n",
      "Saving epoch 14 for SWA (weight averaging - late epoch)\n",
      "\n",
      "==================================================\n",
      "Start training epoch 15/15\n",
      "==================================================\n",
      "Training: 100% 4/4 [00:01<00:00,  2.05batch/s, loss=0.54379]\n",
      "\n",
      "Validating on development set...\n",
      "Evaluation: 100% 1/1 [00:00<00:00,  4.27batch/s]\n",
      "Scores saved to exp_result/FakeorReal_audio_SEResNet_rand_ep15_bs32_feat1/metrics/dev_score.txt\n",
      "\\nCLASSIFICATION RESULTS\\n\\tEER\\t\\t= 10.555555556 % (Equal error rate)\\n\\tAccuracy\\t=  92.8571 % (Classification accuracy at EER threshold)\\nDONE.\n",
      "Loss:0.54379, dev_eer: 10.556, dev_acc: 92.86%\n",
      "Saving epoch 15 for SWA (weight averaging - late epoch)\n",
      "Start final evaluation\n",
      "Applying Stochastic Weight Averaging (SWA) - averaging 8 model snapshots\n",
      "Evaluation: 100% 1/1 [00:00<00:00,  3.05batch/s]\n",
      "Scores saved to exp_result/FakeorReal_audio_SEResNet_rand_ep15_bs32_feat1/eval_scores_using_best_dev_model.txt\n",
      "\\nCLASSIFICATION RESULTS\\n\\tEER\\t\\t= 50.000000000 % (Equal error rate)\\n\\tAccuracy\\t=  40.0000 % (Classification accuracy at EER threshold)\\n\n",
      "======================================================================\n",
      "SAVING METRICS AND VISUALIZATIONS\n",
      "======================================================================\n",
      "[OK] Metrics saved to exp_result/FakeorReal_audio_SEResNet_rand_ep15_bs32_feat1/metrics/metrics.json\n",
      "[OK] Metrics CSV saved to exp_result/FakeorReal_audio_SEResNet_rand_ep15_bs32_feat1/metrics/metrics.csv\n",
      "[OK] Training metrics plot saved to exp_result/FakeorReal_audio_SEResNet_rand_ep15_bs32_feat1/metrics/training_metrics.png\n",
      "[OK] Final metrics plot saved to exp_result/FakeorReal_audio_SEResNet_rand_ep15_bs32_feat1/metrics/final_metrics.png\n",
      "[OK] Metrics summary saved to exp_result/FakeorReal_audio_SEResNet_rand_ep15_bs32_feat1/metrics/metrics_summary.txt\n",
      "======================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "                    GENERATING COMPREHENSIVE VISUALIZATIONS\n",
      "================================================================================\n",
      "\n",
      "[INFO] Collecting predictions for visualization...\n",
      "\n",
      "[INFO] Development Set:\n",
      "Collecting predictions: 100% 1/1 [00:00<00:00,  4.64batch/s]\n",
      "\n",
      "üìä Generating dev visualizations...\n",
      "‚úì Confusion matrix saved to exp_result/FakeorReal_audio_SEResNet_rand_ep15_bs32_feat1/metrics/confusion_matrix_dev.png\n",
      "‚úì ROC curve saved to exp_result/FakeorReal_audio_SEResNet_rand_ep15_bs32_feat1/metrics/roc_curve_dev.png\n",
      "‚úì Classification report saved to exp_result/FakeorReal_audio_SEResNet_rand_ep15_bs32_feat1/metrics/classification_report_dev.txt\n",
      "\n",
      "[INFO] Evaluation Set:\n",
      "Collecting predictions: 100% 1/1 [00:00<00:00, 11.84batch/s]\n",
      "\n",
      "üìä Generating eval visualizations...\n",
      "‚úì Confusion matrix saved to exp_result/FakeorReal_audio_SEResNet_rand_ep15_bs32_feat1/metrics/confusion_matrix_eval.png\n",
      "‚úì ROC curve saved to exp_result/FakeorReal_audio_SEResNet_rand_ep15_bs32_feat1/metrics/roc_curve_eval.png\n",
      "‚úì Classification report saved to exp_result/FakeorReal_audio_SEResNet_rand_ep15_bs32_feat1/metrics/classification_report_eval.txt\n",
      "‚úì Accuracy comparison saved to exp_result/FakeorReal_audio_SEResNet_rand_ep15_bs32_feat1/metrics/accuracy_comparison.png\n",
      "\n",
      "================================================================================\n",
      "                         FINAL TRAINING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "[TRAINING] TRAINING METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "  Initial Loss: 1.16081\n",
      "  Final Loss:   0.54379\n",
      "  Min Loss:     0.19090\n",
      "\n",
      "[DEV] DEVELOPMENT SET METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "  Best Dev EER:     10.5556%\n",
      "  Best Dev Accuracy: 92.86%\n",
      "\n",
      "[EVAL] EVALUATION SET METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "  EER:       50.0000%\n",
      "  ROC AUC:   0.3333\n",
      "  Accuracy:  40.00%\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[OK] Final summary saved to exp_result/FakeorReal_audio_SEResNet_rand_ep15_bs32_feat1/metrics/final_summary.txt\n",
      "\n",
      "[OK] All visualizations generated successfully!\n",
      "================================================================================\n",
      "\n",
      "Exp FIN. EER: 50.000, Accuracy: 40.00%\n"
     ]
    }
   ],
   "source": [
    "!python -u main.py --config config/SEResNet.conf --feature_type 1 --dataset 2 --epochs 15 --random_noise --weight_avg --eval_best --data_subset 0.01 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNet-B2 with Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -u main.py --config config/EfficientNetB2_Attention.conf --feature_type 1 --dataset 2 --epochs 20 --random_noise --weight_avg --eval_best "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RawNet3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -u main.py --config config/RawNet3.conf --feature_type 0 --dataset 2 --epochs 15 --random_noise --weight_avg --eval_best "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AASIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -u main.py --config config/AASIST.conf --feature_type 0 --dataset 2 --epochs 30 --random_noise --weight_avg --eval_best "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Output Structure\n",
    "\n",
    "After training, results are saved in `exp_result/`:\n",
    "\n",
    "```\n",
    "exp_result/\n",
    "‚îî‚îÄ‚îÄ <dataset>_<track>_<model>_<flags>_ep<epochs>_bs<batch>_feat<feature>/\n",
    "    ‚îú‚îÄ‚îÄ config.conf              # Copy of training config\n",
    "    ‚îú‚îÄ‚îÄ weights/                 # Model checkpoints\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ best.pth            # Best model (lowest dev EER)\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ swa.pth             # SWA averaged model\n",
    "    ‚îú‚îÄ‚îÄ metrics/                 # Training metrics\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ epoch_metrics.json  # Per-epoch metrics\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ final_summary.json  # Final results\n",
    "    ‚îú‚îÄ‚îÄ metric_log.txt          # Training log\n",
    "    ‚îú‚îÄ‚îÄ evaluation_results.txt  # Final evaluation\n",
    "    ‚îî‚îÄ‚îÄ events.out.*            # TensorBoard logs\n",
    "```\n",
    "\n",
    "### Metrics\n",
    "\n",
    "| Metric       | Description                                    |\n",
    "| ------------ | ---------------------------------------------- |\n",
    "| **EER**      | Equal Error Rate                               |\n",
    "| **Accuracy** | Classification accuracy                        |\n",
    "| **t-DCF**    | Tandem Detection Cost Function (ASVspoof only) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Step 5: Evaluate Model\n",
    "\n",
    "Evaluate a trained model on the test set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace path with your trained model\n",
    "#!python main.py --config config/LCNN.conf --dataset 2 --feature_type 1 --eval --eval_model_weights ./exp_result/<your_model_folder>/weights/best.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üí° Common Issues\n",
    "\n",
    "| Issue           | Solution                                                          |\n",
    "| --------------- | ----------------------------------------------------------------- |\n",
    "| Out of memory   | Reduce `--batch_size 16` or `--batch_size 8`                      |\n",
    "| Slow training   | Mixed precision is enabled by default (`use_amp: true` in config) |\n",
    "| Need quick test | Use `--data_subset 0.1` to train on 10% of data                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß™ Step 6: Feature & Model Experiments\n",
    "\n",
    "Run experiments with different feature types and model configurations to find the best combination.\n",
    "\n",
    "### Feature Type Reference\n",
    "\n",
    "| Feature             | Best For            | Models                       |\n",
    "| ------------------- | ------------------- | ---------------------------- |\n",
    "| `0` Raw             | End-to-end learning | RawNet3, AASIST              |\n",
    "| `1` Mel-spectrogram | General purpose     | LCNN, EfficientNet, SEResNet |\n",
    "| `2` LFCC            | Speech features     | LCNN, SEResNet               |\n",
    "| `3` MFCC            | Traditional speech  | All spectrogram models       |\n",
    "| `4` CQT             | Harmonic analysis   | LCNN, EfficientNet           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: LCNN with CQT Features\n",
    "\n",
    "CQT (Constant-Q Transform) provides excellent harmonic resolution for detecting synthesis artifacts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -u main.py --config config/LCNN.conf --feature_type 4 --dataset 2 --epochs 20 --random_noise --weight_avg --eval_best "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: LCNN with LFCC Features\n",
    "\n",
    "LFCC (Linear Frequency Cepstral Coefficients) is widely used for spoofing detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -u main.py --config config/LCNN.conf --feature_type 2 --dataset 2 --epochs 20 --random_noise --weight_avg --eval_best "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: EfficientNet-B2 with CQT Features\n",
    "\n",
    "EfficientNet with CQT can capture fine-grained frequency patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -u main.py --config config/EfficientNetB2_Attention.conf --feature_type 4 --dataset 2 --epochs 20 --random_noise --weight_avg --eval_best "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4: SEResNet with MFCC Features\n",
    "\n",
    "SEResNet with MFCC for traditional speech feature analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -u main.py --config config/SEResNet.conf --feature_type 3 --dataset 2 --epochs 15 --random_noise --weight_avg --eval_best "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5: AASIST-L (Large) with Raw Waveform\n",
    "\n",
    "AASIST-L is a larger version with more capacity for complex patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -u main.py --config config/AASIST-L.conf --feature_type 0 --dataset 2 --epochs 30 --random_noise --weight_avg --eval_best "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiment 6: SimpleCNN Baseline with Mel-spectrogram\n",
    "\n",
    "A lightweight baseline model for quick comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -u main.py --config config/SimpleCNN.conf --feature_type 1 --dataset 2 --epochs 25 --random_noise --weight_avg --eval_best "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 7: LCNN Large with LFCC Features\n",
    "\n",
    "Larger LCNN model with LFCC for better spoofing artifact detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -u main.py --config config/LCNN_Large.conf --feature_type 2 --dataset 2 --epochs 20 --random_noise --weight_avg --eval_best "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìà Step 7: Compare All Results\n",
    "\n",
    "After running experiments, visualize and compare all model results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all trained models\n",
    "!python visualize_results.py --path \"exp_result/*/metrics\" --compare --show-summary --output ./comparison_plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Comparison Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from pathlib import Path\n",
    "\n",
    "# Display comparison plot if it exists\n",
    "comparison_plot = Path(\"./comparison_plots/model_comparison.png\")\n",
    "if comparison_plot.exists():\n",
    "    display(Image(filename=str(comparison_plot)))\n",
    "else:\n",
    "    print(\"Run the comparison command above first to generate plots.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Experiment Summary Table\n",
    "\n",
    "After completing all experiments, fill in the results:\n",
    "\n",
    "| Experiment | Model           | Feature      | Epochs | Best EER (%) | Best Accuracy (%) |\n",
    "| ---------- | --------------- | ------------ | ------ | ------------ | ----------------- |\n",
    "| Baseline 1 | LCNN            | Mel-spec (1) | 20     | -            | -                 |\n",
    "| Baseline 2 | EfficientNet-B2 | Mel-spec (1) | 20     | -            | -                 |\n",
    "| Baseline 3 | RawNet3         | Raw (0)      | 15     | -            | -                 |\n",
    "| Baseline 4 | AASIST          | Raw (0)      | 30     | -            | -                 |\n",
    "| Exp 1      | LCNN            | CQT (4)      | 20     | -            | -                 |\n",
    "| Exp 2      | LCNN            | LFCC (2)     | 20     | -            | -                 |\n",
    "| Exp 3      | EfficientNet-B2 | CQT (4)      | 20     | -            | -                 |\n",
    "| Exp 4      | SEResNet        | MFCC (3)     | 15     | -            | -                 |\n",
    "| Exp 5      | AASIST-L        | Raw (0)      | 30     | -            | -                 |\n",
    "| Exp 6      | SimpleCNN       | Mel-spec (1) | 25     | -            | -                 |\n",
    "| Exp 7      | LCNN Large      | LFCC (2)     | 20     | -            | -                 |\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "- Lower EER is better (0% = perfect)\n",
    "- Higher Accuracy is better (100% = perfect)\n",
    "- CQT and LFCC features often work well for spoofing detection\n",
    "- Raw waveform models (RawNet3, AASIST) learn features automatically\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
